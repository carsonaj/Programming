import torch

def encoding_function(num_categories):
    """one hot encoder, given the number of categories returns a one-hot 
    encoder function the eats tensors of size (N,1) (where N is batch size 
    and each row corresonds to one numerical value) and spits out a 
    tensors of size (N,num_categories) where each row corresponds to 
    the one hot encoding associated to the numerical value"""

    def encoder(vals):
        """vals of size (N,1), each row is a one value in 
        [0,num_categories-1]"""
        batch_size = vals.size(0)
        encoded_vals = torch.zeros((batch_size, num_categories))
        
        for i in range(batch_size):
            val = vals[i].item()
            encoded_vals[i][val] = 1
            
        return encoded_vals
        
    return encoder 
        
class NN(torch.nn.Module):
    
    def __init__(self, layer_shapes, activation_functions):
        """layer_shapes is a list, activation_functions is a ModuleList"""
        super(NN, self).__init__()
        assert len(layer_shapes) == len(activation_functions) + 1
        self.layer_shapes = layer_shapes
        self.activation_functions = activation_functions
        
        linear_functions = torch.nn.ModuleList()
        for i in range(len(self.layer_shapes)-1):
            linear_functions.append(torch.nn.Linear(
                    self.layer_shapes[i], self.layer_shapes[i+1]))
        
        self.linear_functions = linear_functions

    def forward(self, x):
        y = x
        for i in range(len(self.layer_shapes)-1):
            assert y.shape[1] == self.layer_shapes[i]
            y = self.activation_functions[i](self.linear_functions[i](y))
        return y
